{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd5345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag data triall1-3eb8b421' at:\n",
      "https://eu.smith.langchain.com/o/12537708-62c5-486d-9e38-b7853d327780/datasets/89ac1e0b-7261-4a89-936c-023a64cd06bd/compare?selectedSessions=7bbed207-53d3-468f-baaf-4316d941ffa6\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad71cc4a1f7408ab7692fe26810ba1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.input</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.similarity</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who were the final project presentations showc...</td>\n",
       "      <td>The final week project presentations were show...</td>\n",
       "      <td>None</td>\n",
       "      <td>The project was showcased to IBMâ€™s General Man...</td>\n",
       "      <td>9</td>\n",
       "      <td>5.049451</td>\n",
       "      <td>0eff0c50-51c0-4ab4-8ad9-8adcae45cfbd</td>\n",
       "      <td>00c4d827-5a8c-4d5b-b8ff-deeaf2cf32a9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How did you solve the problem of intent recogn...</td>\n",
       "      <td>I used a router chain to solve the issue of in...</td>\n",
       "      <td>None</td>\n",
       "      <td>Intent recognition was solved by using a route...</td>\n",
       "      <td>8</td>\n",
       "      <td>1.407540</td>\n",
       "      <td>25e70f13-204b-4eb0-b634-e7572acbb388</td>\n",
       "      <td>d35f6e5f-fdbb-400e-9c71-b84c856f7e44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was the main objective of the project?</td>\n",
       "      <td>The primary objectives of the project were:\\n\\...</td>\n",
       "      <td>None</td>\n",
       "      <td>The main objective was to enhance the travel e...</td>\n",
       "      <td>8</td>\n",
       "      <td>2.232957</td>\n",
       "      <td>26f56aa7-49d9-4566-9424-56bb82bbb587</td>\n",
       "      <td>5aa59397-3bbf-48b8-9639-b458f9fcd1cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What challenges did you face with LLMs?</td>\n",
       "      <td>According to the context, I faced two major ch...</td>\n",
       "      <td>None</td>\n",
       "      <td>One major challenge was LLM hallucination, whe...</td>\n",
       "      <td>8</td>\n",
       "      <td>2.287549</td>\n",
       "      <td>36ef975b-e8f7-49d5-950e-4735cf3449d8</td>\n",
       "      <td>00956ddc-9338-4ba8-9bf4-f150fe8440ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How did your university studies differ from re...</td>\n",
       "      <td>Based on the context provided, it seems that m...</td>\n",
       "      <td>None</td>\n",
       "      <td>University focused on theoretical foundations ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2.908452</td>\n",
       "      <td>4b8eb673-2819-4d7a-9d7f-30c4f2452e0c</td>\n",
       "      <td>f2231ffb-4b5c-48f2-bec6-727c2970e7d8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How was user session memory handled?</td>\n",
       "      <td>A buffer memory system was implemented to hand...</td>\n",
       "      <td>None</td>\n",
       "      <td>A buffer memory approach was used, storing onl...</td>\n",
       "      <td>9</td>\n",
       "      <td>1.437968</td>\n",
       "      <td>77766f65-6957-4c64-9152-33eab3b3393b</td>\n",
       "      <td>1e70d633-11b5-444d-8a3f-a3eec940099c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What skills did you gain during the project?</td>\n",
       "      <td>I gained extensive hands-on experience and dev...</td>\n",
       "      <td>None</td>\n",
       "      <td>Skills gained included LangChain, LLMs, RAG me...</td>\n",
       "      <td>8</td>\n",
       "      <td>1.612615</td>\n",
       "      <td>92bb5916-0f7a-4b54-b163-5fe943461ef2</td>\n",
       "      <td>a273fc10-5db2-4daa-b43d-a30a0d46ed80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the trip planning module?</td>\n",
       "      <td>The Trip Planning Module is a feature of the c...</td>\n",
       "      <td>None</td>\n",
       "      <td>The trip planning module suggests itineraries,...</td>\n",
       "      <td>9</td>\n",
       "      <td>1.770625</td>\n",
       "      <td>a98a2d47-9fbb-478b-9447-ac6a6106c5c5</td>\n",
       "      <td>c454b9b6-c80e-43b1-b953-20cf0803b2f2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Which technologies did the team use for cloud ...</td>\n",
       "      <td>The team used Google Cloud for training prompt...</td>\n",
       "      <td>None</td>\n",
       "      <td>The team utilized Google Cloud for training pr...</td>\n",
       "      <td>8</td>\n",
       "      <td>1.819446</td>\n",
       "      <td>e47aaa14-59ff-4f18-9c73-94ce1d3de881</td>\n",
       "      <td>b02bddfc-2f19-49b3-b22f-e54adfecc056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the key features of the chatbot?</td>\n",
       "      <td>Based on the context, the key features of the ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Key features included multilingual support, tr...</td>\n",
       "      <td>6</td>\n",
       "      <td>4.336784</td>\n",
       "      <td>f7cf87d4-ffb3-467d-9e73-bcb88d57d59b</td>\n",
       "      <td>2a5507bf-6bdd-4271-83d5-37aa4281cecd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults rag data triall1-3eb8b421>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate, Client\n",
    "from difflib import SequenceMatcher\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "client = Client()\n",
    "dataset_name = \"Rag data\"\n",
    "\n",
    "# Initialize Ollama embeddings\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.1\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# Path to your vectorstore\n",
    "vectorstore_path = \"C:\\\\Users\\\\PC\\\\Desktop\\\\rag for interview\\\\pdf_vectorstore\"\n",
    "\n",
    "def retrieve_documents(question):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from the FAISS index based on the question\n",
    "    \n",
    "    Args:\n",
    "        question: The query to search for in the vector store\n",
    "        \n",
    "    Returns:\n",
    "        List of documents relevant to the question\n",
    "    \"\"\"\n",
    "    # Load the existing vector store\n",
    "    vectorstore = FAISS.load_local(vectorstore_path, embedding_model,allow_dangerous_deserialization=True)\n",
    "    \n",
    "    # Retrieve documents from the vector store\n",
    "    docs = vectorstore.similarity_search(question, k=4)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def generate_response(question, documents):\n",
    "    \"\"\"\n",
    "    Generate a response to the question using the retrieved documents\n",
    "    \n",
    "    Args:\n",
    "        question: The query to answer\n",
    "        documents: The retrieved documents to use as context\n",
    "        \n",
    "    Returns:\n",
    "        Generated answer to the question\n",
    "    \"\"\"\n",
    "    # Create a context string from the documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    # Create a prompt template\n",
    "    prompt_template = \"\"\"\n",
    "    You are a helpful assistant. Use the following context to answer the question.\n",
    "    If you don't know the answer based on the context, just say you don't know.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    # Create an LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    \n",
    "    # Run the chain\n",
    "    response = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    return response[\"text\"]\n",
    "\n",
    "def langsmith_rag(question):\n",
    "    \"\"\"\n",
    "    RAG pipeline function that retrieves documents and generates a response\n",
    "    \n",
    "    Args:\n",
    "        question: The query to answer\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with the generated output\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    documents = retrieve_documents(question)\n",
    "    \n",
    "    # Generate response\n",
    "    answer = generate_response(question, documents)\n",
    "    \n",
    "    # Return in the format expected by LangSmith\n",
    "    return {\"output\": answer}\n",
    "\n",
    "def similarity_score(reference_outputs: dict, outputs: dict) -> dict:\n",
    "    reference = reference_outputs[\"output\"]\n",
    "    prediction = outputs[\"output\"]\n",
    "    prompt_template = \"\"\"\n",
    "    You are an AI judge evaluating two responses to the same question. Your task is to rate how similar the two answers are in meaning, even if the wording differs. The rating should reflect how well the answers convey the same idea or concept.\n",
    "\n",
    "Consider factors such as:\n",
    "\n",
    "Whether the core message or intent of both answers is aligned.\n",
    "\n",
    "How closely the answers express the same information, even if they use different words or phrasing.\n",
    "\n",
    "Differences in tone, detail, and structure should not heavily influence the meaning similarity.\n",
    "\n",
    "Please assign a score from 0 to 10, where:\n",
    "\n",
    "0 means the answers are completely different in meaning.\n",
    "\n",
    "10 means the answers are identical in meaning.\n",
    "ONLY RESPOND WITH THE SCORE, DO NOT ADD ANY OTHER TEXT. Even if the 2 answers are exact match, do not add any other text.\n",
    "for example\n",
    "reference output: hello this is karim\n",
    "output: hello this is karim\n",
    "Score: 10\n",
    "do not add 'score' only respond with a number \n",
    "\n",
    "reference output: {reference}\n",
    "\n",
    "output : {output}\n",
    "\n",
    "Score:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"reference\", \"output\"]\n",
    "    )\n",
    "    \n",
    "    # Create an LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    \n",
    "    # Run the chain\n",
    "    response = chain.invoke({\"reference\": reference, \"output\": prediction})\n",
    "    \n",
    "    return {\"key\": \"similarity\", \"score\": int(response[\"text\"])}\n",
    "\n",
    "   \n",
    "\n",
    "def target_function(inputs: dict):\n",
    "    return langsmith_rag(inputs[\"input\"])\n",
    "\n",
    "# Only run evaluation if this file is executed directly\n",
    "\n",
    "evaluate(\n",
    "        target_function,\n",
    "        data=dataset_name,\n",
    "        evaluators=[similarity_score],\n",
    "        experiment_prefix=\"rag data triall1\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2c38b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
